{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content of this file :\n",
    "\n",
    "-   After we generated the instDiffInput in our image generation pipeline until gen_paste_annotation_from_plan.py\n",
    "-   instDiffInput is passed through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from configurationLoader import returnRepoConfig\n",
    "repoConfig = returnRepoConfig(\"control_net_gen_cfg.yaml\")\n",
    "from share import *\n",
    "import config\n",
    "import os\n",
    "import cv2\n",
    "import einops\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.uniformer import UniformerDetector\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(json_file_path):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer_one_image(input_image, \n",
    "                    prompt, \n",
    "                    a_prompt, \n",
    "                    n_prompt, \n",
    "                    num_samples, \n",
    "                    image_resolution, \n",
    "                    detect_resolution, \n",
    "                    ddim_steps, \n",
    "                    guess_mode, \n",
    "                    strength, \n",
    "                    scale, \n",
    "                    seed, \n",
    "                    eta,\n",
    "                    model,\n",
    "                    ddim_sampler\n",
    "                ):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #print(f'input_image max: {input_image.max()}, input_image min: {input_image.min()}, input_image.shape: {input_image.shape}, input_image.dtype: {input_image.dtype}')\n",
    "        #print()\n",
    "        input_image = HWC3(input_image)\n",
    "        #detected_map = apply_uniformer(resize_image(input_image, detect_resolution))\n",
    "        img = resize_image(input_image, image_resolution)\n",
    "        H, W, C = img.shape\n",
    "\n",
    "        detected_map = cv2.resize(img, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n",
    "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "        control = einops.rearrange(control, 'b h w c -> b c h w').clone()\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "        #seed_everything(seed)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        cond = {\"c_concat\": [control], \"c_crossattn\": [model.get_learned_conditioning([prompt + ', ' + a_prompt] * num_samples)]}\n",
    "        un_cond = {\"c_concat\": None if guess_mode else [control], \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)]}\n",
    "        shape = (4, H // 8, W // 8)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "        model.control_scales = [strength * (0.825 ** float(12 - i)) for i in range(13)] if guess_mode else ([strength] * 13)  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "        samples, intermediates = ddim_sampler.sample(ddim_steps, num_samples,\n",
    "                                                     shape, cond, verbose=False, eta=eta,\n",
    "                                                     unconditional_guidance_scale=scale,\n",
    "                                                     unconditional_conditioning=un_cond)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = (einops.rearrange(x_samples, 'b c h w -> b h w c') * 127.5 + 127.5).cpu().numpy().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        results = [x_samples[i] for i in range(num_samples)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading and preparation\n",
    "apply_uniformer = UniformerDetector()\n",
    "model = create_model(repoConfig.inference.model_yaml_path).cpu()\n",
    "model.load_state_dict(load_state_dict(repoConfig.inference.trained_controlnet_model, location='cuda'))\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)\n",
    "\n",
    "control_net_captions = read_json_file(repoConfig.inference.control_net_captions_path)\n",
    "\n",
    "counter = 0\n",
    "#Per image and it's caption info, create an inferred image\n",
    "for image_info in tqdm(control_net_captions):\n",
    "\n",
    "    if counter == 10:\n",
    "        break\n",
    "    counter += 1\n",
    "\n",
    "    segmented_img_path = image_info['source']\n",
    "    inference_results = infer_one_image(\n",
    "        input_image = cv2.imread(segmented_img_path),\n",
    "        prompt = image_info['prompt'],\n",
    "         model = model,\n",
    "        ddim_sampler=ddim_sampler,\n",
    "        \n",
    "        a_prompt = repoConfig.inference.controls.a_prompt,\n",
    "        n_prompt = repoConfig.inference.controls.n_prompt,\n",
    "        num_samples = repoConfig.inference.controls.num_samples,\n",
    "        image_resolution = repoConfig.inference.controls.image_resolution,\n",
    "        detect_resolution = repoConfig.inference.controls.detect_resolution,\n",
    "        ddim_steps = repoConfig.inference.controls.ddim_steps,\n",
    "        guess_mode = repoConfig.inference.controls.guess_mode,\n",
    "        strength = repoConfig.inference.controls.strength,\n",
    "        scale = repoConfig.inference.controls.scale,\n",
    "        seed = repoConfig.inference.controls.seed,\n",
    "        eta = repoConfig.inference.controls.eta\n",
    "    )\n",
    "\n",
    "    # Save the inferred images\n",
    "    output_path = os.path.join(repoConfig.inference.output_path, os.path.basename(segmented_img_path))\n",
    "    \n",
    "    #Writing the first sample for now, it's modular for later\n",
    "    cv2.imwrite(output_path, inference_results[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
